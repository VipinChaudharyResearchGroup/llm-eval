{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/vstor/CSE_CSDS_VXC204/mxh1029/envs/conda/conda-dir/envs/g124/bin/python\n",
      "Python 3.12.3\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Apr_17_19:19:55_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.40\n",
      "Build cuda_12.5.r12.5/compiler.34177558_0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "which python\n",
    "python --version\n",
    "nvcc --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import GPT2LMHeadModel\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Device name: NVIDIA A100-SXM4-80GB\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    device: torch.device\n",
    "    seed: int\n",
    "    cache_dir: Path\n",
    "    base_dir: Path\n",
    "\n",
    "\n",
    "def init(seed: int = None) -> Config:\n",
    "    \"\"\"\n",
    "    Initialize the environment settings for a machine learning project.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed for random number generators to ensure reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Config: A frozen dataclass containing the configuration settings.\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA is available\")\n",
    "        print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Device count:\", torch.cuda.device_count())\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available\")\n",
    "\n",
    "    # Set Hugging Face environment variables\n",
    "    hf_telemetry = 1  # Set to 1 to disable telemetry\n",
    "    os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = str(hf_telemetry)\n",
    "\n",
    "    # Ensure required environment variables are set\n",
    "    cs_bash = os.getenv(\"CS_BASH\")\n",
    "    cs_home = os.getenv(\"CS_HOME\")\n",
    "    if not cs_bash:\n",
    "        raise EnvironmentError(\"Environment variable CS_BASH is not set\")\n",
    "    if not cs_home:\n",
    "        raise EnvironmentError(\"Environment variable CS_HOME is not set\")\n",
    "\n",
    "    # Set Hugging Face token from environment script\n",
    "    env_path = Path(cs_bash) / \".env.py\"\n",
    "    if env_path.is_file():\n",
    "        with open(env_path, \"r\") as env_file:\n",
    "            env_script = env_file.read()\n",
    "            exec(env_script)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Environment file not found: {env_path}\")\n",
    "\n",
    "    cache_dir = Path(cs_home) / \".cache/misc\"\n",
    "\n",
    "    # Set random seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    notebook_path = Path(cs_home) / \"notebooks\"\n",
    "    if not notebook_path.is_dir():\n",
    "        raise EnvironmentError(f\"Notebook directory not found: {notebook_path}\")\n",
    "\n",
    "    os.chdir(notebook_path)\n",
    "\n",
    "    return Config(device=device, seed=seed, cache_dir=cache_dir, base_dir=notebook_path)\n",
    "\n",
    "\n",
    "conf = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I am learning Transformers from huggungface's transformers library and Andrej Karpathy's nanoGPT + 3b1b's YT series.\n",
    "They all have different names for the same thing. To make sense of it all, here there are different names for the same thing.\n",
    "\n",
    "GPT2 Config: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/configuration_gpt2.py\n",
    "\n",
    "\n",
    "There are 4 types of GPT2 models:\n",
    "- GPT2: default (this file)\n",
    "- GPT2 Medium: n_layer=24, n_head=16, d_model=1024\n",
    "- GPT2 Large: n_layer=36, n_head=20, d_model=1280\n",
    "- GPT2 XL: n_layer=48, n_head=25, d_model=1600\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "\n",
    "    # Vocabulary size = Num of tokens\n",
    "    # This is used to build a lookup table for embeddings. Each token is a row in the table pointing to a the corresponding embedding vector.\n",
    "    vocab_size: int = 50257  # hf\n",
    "    n_vocab: int = vocab_size  # 3b1b\n",
    "\n",
    "    # Word embedding dimension\n",
    "    n_embd: int = 768  # Andrej, hf\n",
    "    embed_dim: int = n_embd  # hf\n",
    "    d_embed: int = n_embd  # 3b1b\n",
    "    # Hidden layer dimension\n",
    "    # First, we have tokens (integers) as the input of the model. Then after the embedding layer, we have embeddings (vectors) which can be seen as hidden states.\n",
    "    # Because of that, it is making sense to call the embedding dimension as the hidden size.\n",
    "    hidden_size = embed_dim  # hf transformers uses both names,\n",
    "\n",
    "    # Number of positional embeddings = Max number of tokens in a sequence\n",
    "    # GPT2 uses an absolute positional embedding. The positional embeddings are added to the token embeddings.\n",
    "    n_positions: int = 1024  # hf\n",
    "    # This should be maximum. GPT2 doesn't use KV cache. So, the inference process starts by the input tokens\n",
    "    # and adds new tokens to the sequence until the max number of tokens.\n",
    "    sequence_len: int = n_positions\n",
    "    max_seq_len: int = n_positions\n",
    "    max_position_embeddings: int = n_positions  # hf, transformers lib,\n",
    "    block_size: int = n_positions  # Andrej\n",
    "\n",
    "    # Number of context tokens = Attention window size. There is no actual windowing in GPT2, so this is the max number of tokens in a sequence.\n",
    "    n_ctx: int = 1024  # hf\n",
    "    ctx_len: int = n_ctx\n",
    "    ctx_size: int = n_ctx\n",
    "\n",
    "    # Number of layers\n",
    "    # Number of GPT2Blocks (in transformers lib)\n",
    "    # These layer are used sequentially. Each layer has a self-attention mechanism and a feedforward neural network.\n",
    "    # In each layer (iteration), there are multiple attention heads. Each head has its own query, key, value matrices.\n",
    "    n_layer: int = 12  # Andrej, hf\n",
    "    num_hidden_layers: int = n_layer  # hf\n",
    "\n",
    "    # Number of attention heads\n",
    "    # They run in parallel. Each head learns different features.\n",
    "    # Dimension of each each can be calculated as d_model / n_head.\n",
    "    n_head: int = 12  # Andrej, hf\n",
    "    num_attention_heads: int = n_head  # hf\n",
    "\n",
    "    # Head size = head dimension\n",
    "    head_size: int = n_embd // n_head\n",
    "\n",
    "    # Query space dimension\n",
    "    query_dim: int = 64\n",
    "    d_query: int = query_dim  # 3b1b\n",
    "\n",
    "    # Value space dimension\n",
    "    value_dim: int = 64\n",
    "    d_value: int = value_dim  # 3b1b\n",
    "\n",
    "    # Key space dimension\n",
    "    key_dim: int = 64\n",
    "    d_key: int = key_dim  # 3b1b\n",
    "\n",
    "    # Dropout and layer norm\n",
    "    attn_pdrop: float = 0.1  # hf\n",
    "    embd_pdrop: float = 0.1  # hf\n",
    "    layer_norm_epsilon: float = 1e-5  # hf\n",
    "    resid_pdrop: float = 0.1  # hf\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert (\n",
    "            self.n_embd % self.n_head == 0\n",
    "        ), \"Embedding dimension must be divisible by the number of heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_size = config.head_size\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "        self.c_attn = nn.Linear(\n",
    "            in_features=config.n_embd, out_features=3 * config.n_embd\n",
    "        )\n",
    "\n",
    "        self.c_proj = nn.Linear(\n",
    "            in_features=config.n_embd, out_features=config.n_embd, bias=True\n",
    "        )\n",
    "\n",
    "        # self.register_buffer(\n",
    "        #     \"mask\", torch.tril(torch.ones(config.sequence_len, config.sequence_len))\n",
    "        # )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)))\n",
    "\n",
    "    def _attention(self, Q, K, V, batch_size, seq_len, n_embd):\n",
    "        \"\"\"\n",
    "        Compute the attention output.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): The query tensor of shape (batch_size, n_head, seq_len, head_size).\n",
    "            K (torch.Tensor): The key tensor of shape (batch_size, n_head, seq_len, head_size).\n",
    "            V (torch.Tensor): The value tensor of shape (batch_size, n_head, seq_len, head_size).\n",
    "            batch_size (int): The batch size.\n",
    "            seq_len (int): The sequence length.\n",
    "            n_embd (int): The embedding dimension.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        K_tr = K.transpose(-2, -1)  # (batch_size, n_head, head_size, seq_len)\n",
    "        # -2 and -1 are the last two dimensions, don't touch the batch_size dimension\n",
    "\n",
    "        attention_scores = (\n",
    "            Q @ K_tr\n",
    "        )  # (batch_size, n_head, seq_len, seq_len) or (B, H, T, T)\n",
    "\n",
    "        attention_scores_normalized = attention_scores / (\n",
    "            self.head_size**0.5\n",
    "        )  # Normalization by square root of key dimension\n",
    "\n",
    "        T = seq_len\n",
    "\n",
    "        # Exp1: Placement\n",
    "\n",
    "        ## out-of-place\n",
    "        masked_attention_scores_normalized = attention_scores_normalized.masked_fill(\n",
    "            self.bias[:, :, :T, :T] == 0, float(\"-inf\")\n",
    "        )\n",
    "        attention_weights = F.softmax(\n",
    "            masked_attention_scores_normalized, dim=-1\n",
    "        )  # it calculates the softmax for each row in the last dimension\n",
    "\n",
    "        ## in-place\n",
    "        # attention_scores_normalized.masked_fill_(\n",
    "        #     self.bias[:, :, :T, :T] == 0, float(\"-inf\")\n",
    "        # )\n",
    "        # attention_weights = F.softmax(\n",
    "        #     attention_scores_normalized, dim=-1\n",
    "        # )  # it calculates the softmax for each row in the last dimension\n",
    "\n",
    "        attention = attention_weights @ V  # (batch_size, n_head, seq_len, head_size)\n",
    "\n",
    "        # Exp2: Contiguity\n",
    "        # transpose returns a non-contiguous tensor. To make it better for memory access, we use contiguous()\n",
    "\n",
    "        # Contiguous tensor\n",
    "        attention_output = (\n",
    "            attention.transpose(1, 2).contiguous().view(batch_size, seq_len, n_embd)\n",
    "        )\n",
    "\n",
    "        ## Non-contiguous tensor\n",
    "        # attention_output = attention.transpose(1, 2).reshape(batch_size, seq_len, n_embd)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embd = x.size()\n",
    "\n",
    "        QKV = self.c_attn(x)\n",
    "        # print(\"QKV shape:\", QKV.shape)\n",
    "        Q, K, V = QKV.split(self.n_embd, dim=2)\n",
    "        # print(\"Q shape:\", Q.shape)\n",
    "        # print(\"K shape:\", K.shape)\n",
    "        # print(\"V shape:\", V.shape)\n",
    "\n",
    "        Q = Q.view(\n",
    "            batch_size, seq_len, self.n_head, n_embd // self.n_head\n",
    "        )  # (batch_size, seq_len, n_head, head_size)\n",
    "        Q = Q.transpose(1, 2)  # (batch_size, n_head, seq_len, head_size)\n",
    "\n",
    "        K = K.view(\n",
    "            batch_size, seq_len, self.n_head, n_embd // self.n_head\n",
    "        )  # (batch_size, seq_len, n_head, head_size)\n",
    "        K = K.transpose(1, 2)\n",
    "\n",
    "        V = V.view(\n",
    "            batch_size, seq_len, self.n_head, n_embd // self.n_head\n",
    "        )  # (batch_size, seq_len, n_head, head_size)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        attention_output = self._attention(Q, K, V, batch_size, seq_len, n_embd)\n",
    "\n",
    "        output = self.c_proj(attention_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class GPT2MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config, layer_idx=None):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, config: GPT2Config, layer_idx=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(normalized_shape=config.n_embd)\n",
    "        self.mlp = GPT2MLP(config, layer_idx=layer_idx)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # input dimension: (batch_size, sequence_length, n_emd)\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(\n",
    "            hidden_states\n",
    "        )  # This is the input to the attention layer\n",
    "        attn_output = self.attn(hidden_states)  # the size is (B, T, n_emd)\n",
    "        hidden_states = residual + attn_output\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_2(hidden_states)  # This is the input to the MLP layer\n",
    "        mlp_output = self.mlp(\n",
    "            hidden_states\n",
    "        )  # or feed_forward_hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + mlp_output\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    model_type: Literal[\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"] = \"gpt2\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: Literal[\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"] = \"gpt2\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "\n",
    "        config = {\n",
    "            \"gpt2\": GPT2Config(),\n",
    "            \"gpt2-medium\": GPT2Config(n_embd=1024, n_head=16, n_layer=24),\n",
    "            \"gpt2-large\": GPT2Config(n_embd=1280, n_head=20, n_layer=36),\n",
    "            \"gpt2-xl\": GPT2Config(n_embd=1600, n_head=25, n_layer=48),\n",
    "        }[model_type]\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                # word token embeddings\n",
    "                wte=nn.Embedding(\n",
    "                    num_embeddings=config.vocab_size, embedding_dim=config.n_embd\n",
    "                ),\n",
    "                # word position embeddings\n",
    "                wpe=nn.Embedding(\n",
    "                    num_embeddings=config.n_positions, embedding_dim=config.n_embd\n",
    "                ),\n",
    "                h=nn.ModuleList(\n",
    "                    [GPT2Block(config, layer_idx=i) for i in range(config.n_layer)]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(\n",
    "            in_features=config.n_embd, out_features=config.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the GPT2 model. The forward pass of the GPT2 model consists of the following steps:\n",
    "        1. Token Embeddings: The input sequence of tokens is passed through the token embeddings layer to get the token embeddings.\n",
    "        2. Position Embeddings: The position embeddings are added to the token embeddings to get the input embeddings.\n",
    "        3. GPT2 Block: The input embeddings are passed through the GPT2 block, which consists of a multi-head self-attention layer and a feed-forward neural network.\n",
    "            3.1. Layer Normalization: The input embeddings are passed through a layer normalization layer.\n",
    "            3.2. Multi-Head Self-Attention: The output of the layer normalization layer is passed through the multi-head self-attention layer to get the attention output.\n",
    "            3.3. Residual Connection: The attention output is added to the input embeddings to get the residual output.\n",
    "            3.4. Layer Normalization: The residual output is passed through a layer normalization layer.\n",
    "            3.5. Feed-Forward Neural Network: The output of the layer normalization layer is passed through a feed-forward neural network to get the feed-forward output.\n",
    "            3.6. Residual Connection: The feed-forward output is added to the residual output to get the output of the GPT2 block.\n",
    "        4. Language Model Head: The output of the GPT2 block is passed through the language model head to get the logits for the next token.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): A tensor of shape (batch_size, sequence_length) and dtype torch.int64 (LongTensor).\n",
    "\n",
    "        \"\"\"\n",
    "        _, sequence_length = input_ids.size()\n",
    "\n",
    "        assert (\n",
    "            sequence_length <= self.config.n_positions\n",
    "        ), \"Sequence length is longer than the maximum position\"\n",
    "\n",
    "        input_embeds = self.transformer.wte(\n",
    "            input_ids\n",
    "        )  # (batch_size, sequence_length, n_emd)\n",
    "\n",
    "        # First this will be tested.\n",
    "        # position_ids = torch.arange(start = 0, end = sequence_length, device=input_ids.device) # (sequence_length)\n",
    "        position_ids = torch.arange(\n",
    "            start=0, end=sequence_length, dtype=torch.long, device=input_ids.device\n",
    "        )  # (sequence_length)\n",
    "\n",
    "        # Another implementation\n",
    "        # position_ids = torch.arange(start = 0, end = sequence_length, dtype=  torch.long,device=input_ids.device) # (sequence_length)\n",
    "        # position_ids = position_ids.expand(batch_size, sequence_length) # (batch_size, sequence_length\n",
    "\n",
    "        position_embeds = self.transformer.wpe(\n",
    "            position_ids\n",
    "        )  # (batch_size, sequence_length, n_emd)\n",
    "\n",
    "        hidden_states = (\n",
    "            input_embeds + position_embeds\n",
    "        )  # (batch_size, sequence_length, n_emd)\n",
    "\n",
    "        x = hidden_states  # (batch_size, sequence_length, n_emd) this is the input to the GPT Block\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls):\n",
    "\n",
    "        model = cls()\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model.model_type)\n",
    "\n",
    "        sd = model.state_dict()\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for key, value in sd.items():\n",
    "                if \"attn.bias\" in key:\n",
    "                    if key.endswith(\"attn.c_attn.bias\"):\n",
    "                        value.copy_(sd_hf[key])\n",
    "                else:\n",
    "                    value_to_copy = (\n",
    "                        sd_hf[key].t()\n",
    "                        if any(key.endswith(suffix) for suffix in transposed)\n",
    "                        else sd_hf[key]\n",
    "                    )\n",
    "                    value.copy_(value_to_copy)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate(\n",
    "    model: nn.Module,\n",
    "    input_ids: torch.Tensor,\n",
    "    method: Literal[\"greedy\", \"top_k\", \"top_p\"] = \"greedy\",\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    "    max_length=30,\n",
    "    max_new_tokens=None,\n",
    "    num_return_sequences=1,\n",
    "):\n",
    "    \"\"\"\n",
    "        Generate a sequence of tokens using the model.\n",
    "        1. Initial Input: The process begins with an initial sequence of tokens represented by input_ids, which typically has a shape (batch_size, sequence_length).\n",
    "        2. Token-by-Token Generation: The model generates new tokens one at a time. After generating each token, it appends the token to the input sequence and uses the updated sequence to generate the next token.\n",
    "        3. Sequence Continuation: This process continues until the sequence reaches a specified maximum length, a stop token is generated, or another stopping criterion is met.\n",
    "\n",
    "    Args:\n",
    "        input_ids (torch.Tensor): A tensor of shape (batch_size, sequence_length) and dtype torch.int64 (LongTensor).\n",
    "        max_length (int): The maximum length of the sequence to be generated.\n",
    "        num_return_sequences (int): The number of independently computed returned sequences for each element in the batch.\n",
    "        do_sample (bool): If set to False greedy decoding is used. Otherwise, sampling is used.\n",
    "        top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filter\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, max_length) and dtype torch.int64 (LongTensor).\n",
    "\n",
    "    \"\"\"\n",
    "    # max_new_token = max_new_token or max_length # refactor this later\n",
    "    # s.t.\n",
    "    # max_new_tokens + input_ids.shape[1] = max_length\n",
    "\n",
    "    input_len = input_ids.shape[1]\n",
    "    num_new_tokens = max(max_new_tokens, max_length - input_len)\n",
    "\n",
    "    model.eval()\n",
    "    model.to(input_ids.device)\n",
    "\n",
    "    for _ in range(num_new_tokens):\n",
    "\n",
    "        logits = model(input_ids)  # (batch_size, sequence_length, vocab_size)\n",
    "        # next_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        next_logits = logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "\n",
    "        if temperature != 1.0:\n",
    "            next_logits /= temperature\n",
    "\n",
    "        next_probs = F.softmax(next_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "        # print(next_probs.sum(dim=-1))   # sum of probabilities should be 1\n",
    "\n",
    "        if method == \"greedy\":\n",
    "            # torch.max returns (values, indices)\n",
    "            # using keepdim=True, no need to unsqueeze the tensor\n",
    "            # _, next_token = torch.max(next_probs, dim=-1, keepdim=True) # the same functionality\n",
    "            _, next_token = next_probs.max(dim=-1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            if method == \"top_k\":\n",
    "                # torch.topk returns (values, indices)\n",
    "                # probs, probs_indices = torch.topk(input=next_probs, k=top_k, dim=-1)\n",
    "                probs, probs_indices = next_probs.topk(k=top_k, dim=-1)\n",
    "\n",
    "            elif method == \"top_p\":\n",
    "\n",
    "                probs, probs_indices = next_probs.sort(\n",
    "                    descending=True, dim=-1\n",
    "                )  # (batch_size, vocab_size)\n",
    "                cumulative_probs = probs.cumsum(dim=-1)  # (batch_size, vocab_size)\n",
    "                mask = cumulative_probs - probs > top_p\n",
    "                probs[mask] = 0.0\n",
    "                # should be normalized since torch.multinomial expects normalized probabilities\n",
    "                # probs.div_(probs.sum(dim = -1, keepdim = True) + 1e-6)\n",
    "                probs.div_(probs.sum(dim=-1, keepdim=True))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid method or missing required argument (top_p or top_k).\"\n",
    "                )\n",
    "\n",
    "            idx_sample = torch.multinomial(input=probs, num_samples=1)\n",
    "\n",
    "            next_token = torch.gather(input=probs_indices, dim=-1, index=idx_sample)\n",
    "\n",
    "        input_ids = torch.cat(\n",
    "            [input_ids, next_token], dim=-1\n",
    "        )  # (batch_size, sequence_length + 1)\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2(\"gpt2\").from_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "# enc = tiktoken.get_encoding(\"r50k_base\")\n",
    "\n",
    "\n",
    "def generate_input_ids(prompt: str, batch_size: int):\n",
    "    tokens = enc.encode(prompt)  # (sequence_length,)\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long, device=conf.device)\n",
    "    input_ids = tokens.unsqueeze(0).repeat(\n",
    "        batch_size, 1\n",
    "    )  # (batch_size, sequence_length)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Simply put, the theory of relativity states that two objects in spacetime are different in the way they reflect different light, so the light from one object to the other must not be lost, leaving a single point of light around and around (and from above the body of the other object, too) so long as they are in the same spot on the ground. But the point-of-light principle is not so simple, because of the way spacetime is set up. Instead, we see objects from different points of the sky — at a given point of the sky, for example — that are different in space as in time. Because this is only possible in the case of\n",
      "\n",
      " Simply put, the theory of relativity states that, for such objects we may not notice them (or that they cannot be detected). It turns out that this conclusion is totally incorrect: the existence of one is simply not always necessarily the truth, and the other is true in some particular respects, such as light being red for us, compared to the other. And that is so for our purposes here.\n",
      "\n",
      "Nevertheless, the two systems have come closer. The real story here is that the two systems have arrived at their original conclusion. For to admit that an object would have to exist in a certain way in order for it to be seen is to accept an incorrect conclusion, namely,\n"
     ]
    }
   ],
   "source": [
    "input_ids = generate_input_ids(\n",
    "    \"Simply put, the theory of relativity states that\", batch_size=2\n",
    ")\n",
    "\n",
    "\n",
    "output_ids = generate(\n",
    "    model=model,\n",
    "    input_ids=input_ids,\n",
    "    method=\"top_k\",\n",
    "    # method=\"top_p\",\n",
    "    # method=\"greedy\",\n",
    "    top_k=50,\n",
    "    # top_p=0.9,\n",
    "    # max_length=30,\n",
    "    max_new_tokens=128,\n",
    "    num_return_sequences=2,\n",
    "    # temperature=0.6,\n",
    ")\n",
    "\n",
    "for decoded_output in enc.decode_batch(output_ids.tolist()):\n",
    "    print(\"-\" * 50, \"\\n\")\n",
    "    print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
