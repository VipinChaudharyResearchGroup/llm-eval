{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/vstor/CSE_CSDS_VXC204/mxh1029/envs/conda/conda-dir/envs/g124/bin/python\n",
      "Python 3.12.3\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Wed_Apr_17_19:19:55_PDT_2024\n",
      "Cuda compilation tools, release 12.5, V12.5.40\n",
      "Build cuda_12.5.r12.5/compiler.34177558_0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "which python\n",
    "python --version\n",
    "nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Device name: NVIDIA A100-SXM4-80GB\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class Config:\n",
    "    device: torch.device\n",
    "    seed: int\n",
    "    cache_dir: Path\n",
    "    base_dir: Path\n",
    "\n",
    "\n",
    "def init(seed: int = None) -> Config:\n",
    "    \"\"\"\n",
    "    Initialize the environment settings for a machine learning project.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): The seed for random number generators to ensure reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Config: A frozen dataclass containing the configuration settings.\n",
    "    \"\"\"\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA is available\")\n",
    "        print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"Device count:\", torch.cuda.device_count())\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA is not available\")\n",
    "\n",
    "    # Set Hugging Face environment variables\n",
    "    hf_telemetry = 1  # Set to 1 to disable telemetry\n",
    "    os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = str(hf_telemetry)\n",
    "\n",
    "    # Ensure required environment variables are set\n",
    "    cs_bash = os.getenv(\"CS_BASH\")\n",
    "    cs_home = os.getenv(\"CS_HOME\")\n",
    "    if not cs_bash:\n",
    "        raise EnvironmentError(\"Environment variable CS_BASH is not set\")\n",
    "    if not cs_home:\n",
    "        raise EnvironmentError(\"Environment variable CS_HOME is not set\")\n",
    "\n",
    "    # Set Hugging Face token from environment script\n",
    "    env_path = Path(cs_bash) / \".env.py\"\n",
    "    if env_path.is_file():\n",
    "        with open(env_path, \"r\") as env_file:\n",
    "            env_script = env_file.read()\n",
    "            exec(env_script)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Environment file not found: {env_path}\")\n",
    "\n",
    "    cache_dir = Path(cs_home) / \".cache/misc\"\n",
    "\n",
    "    # Set random seed for reproducibility if provided\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    \n",
    "    notebook_path = Path(cs_home) / \"notebooks\"\n",
    "    if not notebook_path.is_dir():\n",
    "        raise EnvironmentError(f\"Notebook directory not found: {notebook_path}\")\n",
    "\n",
    "    os.chdir(notebook_path)\n",
    "\n",
    "    return Config(device=device, seed=seed, cache_dir=cache_dir, base_dir=notebook_path)\n",
    "\n",
    "\n",
    "config = init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.base_dir = config.base_dir.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 4096\n",
    "    n_layers: int = 32\n",
    "    n_heads: int = 32\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # Later set in the build method\n",
    "    multiple_of: int = 256\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    # Needed for KV cache\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "    device: str = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # The gamma parameter\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) * (B, Seq_Len, 1) = (B, Seq_Len, Dim)\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (Dim) * (B, Seq_Len, Dim) = (B, Seq_Len, Dim)\n",
    "        return self.weight * self._norm(x.float()).type_as(x)\n",
    "\n",
    "\n",
    "def apply_rotary_embeddings(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
    "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
    "    # Two consecutive values will become a single complex number\n",
    "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
    "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
    "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
    "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
    "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
    "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
    "    x_rotated = x_complex * freqs_complex\n",
    "    # Convert the complex number back to the real number\n",
    "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
    "    x_out = torch.view_as_real(x_rotated)\n",
    "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
    "    x_out = x_out.reshape(*x.shape)\n",
    "    return x_out.type_as(x).to(device)\n",
    "\n",
    "\n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch_size, seq_len, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        # (B, Seq_Len, N_KV_Heads, 1, Head_Dim)\n",
    "        x[:, :, :, None, :]\n",
    "        # (B, Seq_Len, N_KV_Heads, N_Rep, Head_Dim)\n",
    "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
    "        # (B, Seq_Len, N_KV_Heads * N_Rep, Head_Dim)\n",
    "        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Indicates the number of heads for the Keys and Values\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # Indicates the number of heads for the Queries\n",
    "        self.n_heads_q = args.n_heads\n",
    "        # Indicates how many times the Keys and Values should be repeated\n",
    "        self.n_rep = self.n_heads_q // self.n_kv_heads\n",
    "        # Indicates the dimension of each head, that is, the part of the embedding that each head will be responsible for\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        batch_size, seq_len, _ = x.shape  # (B, 1, Dim)\n",
    "\n",
    "        # (B, 1, Dim) -> (B, 1, H_Q * Head_Dim)\n",
    "        xq = self.wq(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xk = self.wk(x)\n",
    "        # (B, 1, Dim) -> (B, 1, H_KV * Head_Dim)\n",
    "        xv = self.wv(x)\n",
    "\n",
    "        # (B, 1, H_Q * Head_Dim) -> (B, 1, H_Q, Head_Dim)\n",
    "        xq = xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xk = xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        # (B, 1, H_KV * Head_Dim) -> (B, 1, H_KV, Head_Dim)\n",
    "        xv = xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) --> (B, 1, H_Q, Head_Dim)\n",
    "        xq = apply_rotary_embeddings(xq, freqs_complex, device=x.device)\n",
    "        # (B, 1, H_KV, Head_Dim) --> (B, 1, H_KV, Head_Dim)\n",
    "        xk = apply_rotary_embeddings(xk, freqs_complex, device=x.device)\n",
    "\n",
    "        # Replace the entry in the cache\n",
    "        self.cache_k[:batch_size, start_pos : start_pos + seq_len] = xk\n",
    "        self.cache_v[:batch_size, start_pos : start_pos + seq_len] = xv\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        keys = self.cache_k[:batch_size, : start_pos + seq_len]\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim)\n",
    "        values = self.cache_v[:batch_size, : start_pos + seq_len]\n",
    "\n",
    "        # Since every group of Q shares the same K and V heads, just repeat the K and V heads for every Q in the same group.\n",
    "\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        keys = repeat_kv(keys, self.n_rep)\n",
    "        # (B, Seq_Len_KV, H_KV, Head_Dim) --> (B, Seq_Len_KV, H_Q, Head_Dim)\n",
    "        values = repeat_kv(values, self.n_rep)\n",
    "\n",
    "        # (B, 1, H_Q, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        xq = xq.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        # (B, Seq_Len_KV, H_Q, Head_Dim) -> (B, H_Q, Seq_Len_KV, Head_Dim)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # (B, H_Q, 1, Head_Dim) @ (B, H_Q, Head_Dim, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        # (B, H_Q, 1, Seq_Len_KV) -> (B, H_Q, 1, Seq_Len_KV)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "\n",
    "        # (B, H_Q, 1, Seq_Len) @ (B, H_Q, Seq_Len_KV, Head_Dim) -> (B, H_Q, 1, Head_Dim)\n",
    "        output = torch.matmul(scores, values)\n",
    "        # (B, H_Q, 1, Head_Dim) -> (B, 1, H_Q, Head_Dim) -> (B, 1, Dim)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        return self.wo(output)  # (B, 1, Dim) -> (B, 1, Dim)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim = 4 * args.dim\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        if args.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(args.ffn_dim_multiplier * hidden_dim)\n",
    "        # Round the hidden_dim to the nearest multiple of the multiple_of parameter\n",
    "        hidden_dim = args.multiple_of * (\n",
    "            (hidden_dim + args.multiple_of - 1) // args.multiple_of\n",
    "        )\n",
    "\n",
    "        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False)\n",
    "        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        swish = F.silu(self.w1(x))\n",
    "        # (B, Seq_Len, Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x_V = self.w3(x)\n",
    "        # (B, Seq_Len, Hidden_Dim) * (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Hidden_Dim)\n",
    "        x = swish * x_V\n",
    "        # (B, Seq_Len, Hidden_Dim) --> (B, Seq_Len, Dim)\n",
    "        x = self.w2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs, layer_id: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.attention = SelfAttention(args)\n",
    "        self.feed_forward = FeedForward(args)\n",
    "\n",
    "        # Normalization BEFORE the attention block\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # Normalization BEFORE the feed forward block\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_complex: torch.Tensor):\n",
    "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_complex)\n",
    "        # (B, Seq_Len, Dim) + (B, Seq_Len, Dim) --> (B, Seq_Len, Dim)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        assert args.vocab_size != -1, \"Vocab size must be set\"\n",
    "\n",
    "        self.args = args\n",
    "        self.vocab_size = args.vocab_size\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [TransformerBlock(args, layer_id) for layer_id in range(args.n_layers)]\n",
    "        )\n",
    "\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "\n",
    "        head_dim = args.dim // args.n_heads\n",
    "        self.freqs_complex = self._precompute_theta_pos_frequencies(\n",
    "            head_dim,\n",
    "            self.args.max_seq_len,\n",
    "        )\n",
    "\n",
    "    def _precompute_theta_pos_frequencies(\n",
    "        self, head_dim: int, max_seq_len: int, theta: float = 10000.0\n",
    "    ):\n",
    "        print(\"precompute_theta_pos_frequencies\")\n",
    "        # This runs only once.\n",
    "\n",
    "        assert head_dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
    "        device = \"cuda\"\n",
    "\n",
    "        # end = max_seq_len * 2 # original implementation in meta GitHub\n",
    "        end = max_seq_len  # I don't know why they are using max_seq_len * 2, but I think it should be max_seq_len, and working.\n",
    "\n",
    "        # torch.arange returns torch.int64\n",
    "        a = 2 * torch.arange(0, head_dim // 2).float().to(device)\n",
    "        # a = [0., 2., ..., 126.], head_dim = 128\n",
    "\n",
    "        freqs = 1.0 / (theta ** (a / head_dim))\n",
    "\n",
    "        position_ids = torch.arange(\n",
    "            start=0, end=end, dtype=torch.long, device=device\n",
    "        ).float()\n",
    "\n",
    "        freqs = torch.outer(position_ids, freqs).float()\n",
    "\n",
    "        freqs_complex = torch.polar(torch.ones_like(freqs), freqs)\n",
    "\n",
    "        return freqs_complex\n",
    "\n",
    "    def forward(self, input_id: torch.Tensor, start_pos: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            input_id (torch.Tensor): (bath_size, 1)\n",
    "            start_pos (int): The starting position of the tokens in the sequence.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # assert input_id.shape[1] == 1, \"Llama uses KV cache, so the input should be a single token\"\n",
    "\n",
    "        h = self.tok_embeddings(input_id)  # (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
    "        freqs_complex = self.freqs_complex  # this is a Tensor\n",
    "\n",
    "        freqs_complex = freqs_complex[start_pos : start_pos + 1]\n",
    "\n",
    "        # Consecutively apply all the encoder layers\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_complex)\n",
    "\n",
    "        h = self.norm(h)\n",
    "\n",
    "        logits = self.output(h).float()\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "\n",
    "def load_tokenizer(\n",
    "    tokenizer_path=f\"{config.base_dir}/.cache/meta_llama2/tokenizer.model\",\n",
    "):\n",
    "\n",
    "    tokenizer = SentencePieceProcessor()\n",
    "    tokenizer.load(tokenizer_path)\n",
    "    vocab_size = tokenizer.vocab_size()\n",
    "\n",
    "    return tokenizer, vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_config(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    llama_path=Path(f\"{config.base_dir}/.cache/meta_llama2/llama-2-7b/\"),\n",
    "):\n",
    "\n",
    "    with open(llama_path / \"params.json\", \"r\") as f:  # Load the params\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args = ModelArgs(\n",
    "        # max_seq_len=max_seq_len,\n",
    "        # max_batch_size=max_batch_size,\n",
    "        device=device,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model_args.vocab_size = vocab_size\n",
    "\n",
    "    return model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(\n",
    "    llama_path=Path(f\"{config.base_dir}/.cache/meta_llama2/llama-2-7b/\"),\n",
    "    model_args=None,\n",
    "):\n",
    "\n",
    "    # https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype\n",
    "    # https://pytorch.org/docs/stable/generated/torch.set_default_device.html#torch.set_default_device\n",
    "    if config.device.type == \"cuda\":\n",
    "        torch.set_default_dtype(torch.float16)\n",
    "        torch.set_default_device(config.device)\n",
    "    else:\n",
    "        torch.set_default_dtype(torch.bfloat16)\n",
    "        torch.set_default_device(\"cpu\")\n",
    "\n",
    "    checkpoints_path = sorted(\n",
    "        (llama_path).glob(\"*.pth\")\n",
    "    )  # For llama-2-7b, there is no need to sort the checkpoints since there is only one checkpoint.\n",
    "\n",
    "    assert len(checkpoints_path) > 0, f\"No checkpoints found in {checkpoints_path}\"\n",
    "\n",
    "    checkpoint = torch.load(\n",
    "        checkpoints_path[0], map_location=\"cpu\"\n",
    "    )  # Load the checkpoint on CPU, [0] since there is only one checkpoint\n",
    "    # Comment from Meta repo: The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
    "    del checkpoint[\"rope.freqs\"]  # Remove the unmatched key\n",
    "\n",
    "    model = Transformer(model_args).to(config.device)\n",
    "\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "    print(\"Checkpoint loaded successfully\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, vocab_size = load_tokenizer()\n",
    "\n",
    "\n",
    "model_args = load_config(vocab_size, config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=4096, n_layers=32, n_heads=32, n_kv_heads=None, vocab_size=32000, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=2048, device=device(type='cuda'))\n"
     ]
    }
   ],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/job.2044264.hpc/ipykernel_3376648/2588513177.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precompute_theta_pos_frequencies\n",
      "Checkpoint loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_args=model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x TransformerBlock(\n",
      "      (attention): SelfAttention(\n",
      "        (wq): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wk): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wv): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (wo): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (w2): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (w3): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      )\n",
      "      (attention_norm): RMSNorm()\n",
      "      (ffn_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
