{
    "model": "meta-llama/Meta-Llama-3-8B",
    "generate_kwargs": {
        "num_return_sequences": 1,
        "max_new_tokens": 24
    },
    "model_config": {
        "vocab_size": 128256,
        "max_position_embeddings": 8192,
        "hidden_size": 4096,
        "intermediate_size": 14336,
        "num_hidden_layers": 32,
        "num_attention_heads": 32,
        "num_key_value_heads": 8,
        "hidden_act": "silu",
        "initializer_range": 0.02,
        "rms_norm_eps": 1e-05,
        "pretraining_tp": 1,
        "use_cache": true,
        "rope_theta": 500000.0,
        "rope_scaling": null,
        "attention_bias": false,
        "attention_dropout": 0.0,
        "mlp_bias": false,
        "return_dict": true,
        "output_hidden_states": false,
        "output_attentions": false,
        "torchscript": false,
        "torch_dtype": "float16",
        "use_bfloat16": false,
        "tf_legacy_loss": false,
        "pruned_heads": {},
        "tie_word_embeddings": false,
        "chunk_size_feed_forward": 0,
        "is_encoder_decoder": false,
        "is_decoder": false,
        "cross_attention_hidden_size": null,
        "add_cross_attention": false,
        "tie_encoder_decoder": false,
        "max_length": 20,
        "min_length": 0,
        "do_sample": false,
        "early_stopping": false,
        "num_beams": 1,
        "num_beam_groups": 1,
        "diversity_penalty": 0.0,
        "temperature": 1.0,
        "top_k": 50,
        "top_p": 1.0,
        "typical_p": 1.0,
        "repetition_penalty": 1.0,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 0,
        "encoder_no_repeat_ngram_size": 0,
        "bad_words_ids": null,
        "num_return_sequences": 1,
        "output_scores": false,
        "return_dict_in_generate": false,
        "forced_bos_token_id": null,
        "forced_eos_token_id": null,
        "remove_invalid_values": false,
        "exponential_decay_length_penalty": null,
        "suppress_tokens": null,
        "begin_suppress_tokens": null,
        "architectures": [
            "LlamaForCausalLM"
        ],
        "finetuning_task": null,
        "id2label": {
            "0": "LABEL_0",
            "1": "LABEL_1"
        },
        "label2id": {
            "LABEL_0": 0,
            "LABEL_1": 1
        },
        "tokenizer_class": null,
        "prefix": null,
        "bos_token_id": 128000,
        "pad_token_id": null,
        "eos_token_id": 128001,
        "sep_token_id": null,
        "decoder_start_token_id": null,
        "task_specific_params": null,
        "problem_type": null,
        "_name_or_path": "meta-llama/Meta-Llama-3-8B",
        "transformers_version": "4.42.3",
        "model_type": "llama"
    },
    "subjects_results": {
        "abstract_algebra": {
            "num_questions_subject": 100,
            "accuracy": 0.24,
            "num_readable_responses": 50,
            "input_tokens_avg": 702.84
        },
        "anatomy": {
            "num_questions_subject": 135,
            "accuracy": 0.5779816513761468,
            "num_readable_responses": 109,
            "input_tokens_avg": 744.9633027522935
        },
        "astronomy": {
            "num_questions_subject": 152,
            "accuracy": 0.7195121951219512,
            "num_readable_responses": 82,
            "input_tokens_avg": 885.7439024390244
        },
        "business_ethics": {
            "num_questions_subject": 100,
            "accuracy": 0.6666666666666666,
            "num_readable_responses": 33,
            "input_tokens_avg": 782.6060606060606
        },
        "clinical_knowledge": {
            "num_questions_subject": 265,
            "accuracy": 0.7534246575342466,
            "num_readable_responses": 146,
            "input_tokens_avg": 647.6095890410959
        },
        "college_biology": {
            "num_questions_subject": 144,
            "accuracy": 0.7073170731707317,
            "num_readable_responses": 123,
            "input_tokens_avg": 806.4471544715448
        },
        "college_chemistry": {
            "num_questions_subject": 100,
            "accuracy": 0.5161290322580645,
            "num_readable_responses": 31,
            "input_tokens_avg": 934.4193548387096
        },
        "college_computer_science": {
            "num_questions_subject": 100,
            "accuracy": 0.41379310344827586,
            "num_readable_responses": 58,
            "input_tokens_avg": 1041.4827586206898
        },
        "college_mathematics": {
            "num_questions_subject": 100,
            "accuracy": 0.3783783783783784,
            "num_readable_responses": 37,
            "input_tokens_avg": 979.3783783783783
        },
        "college_medicine": {
            "num_questions_subject": 173,
            "accuracy": 0.6022727272727273,
            "num_readable_responses": 88,
            "input_tokens_avg": 899.7159090909091
        },
        "college_physics": {
            "num_questions_subject": 102,
            "accuracy": 0.7567567567567568,
            "num_readable_responses": 37,
            "input_tokens_avg": 877.027027027027
        },
        "computer_security": {
            "num_questions_subject": 100,
            "accuracy": 0.7162162162162162,
            "num_readable_responses": 74,
            "input_tokens_avg": 1044.1891891891892
        },
        "conceptual_physics": {
            "num_questions_subject": 235,
            "accuracy": 0.5353535353535354,
            "num_readable_responses": 99,
            "input_tokens_avg": 580.7676767676768
        },
        "econometrics": {
            "num_questions_subject": 114,
            "accuracy": 0.4657534246575342,
            "num_readable_responses": 73,
            "input_tokens_avg": 966.027397260274
        },
        "electrical_engineering": {
            "num_questions_subject": 145,
            "accuracy": 0.5675675675675675,
            "num_readable_responses": 74,
            "input_tokens_avg": 643.1351351351351
        },
        "elementary_mathematics": {
            "num_questions_subject": 378,
            "accuracy": 0.4264705882352941,
            "num_readable_responses": 68,
            "input_tokens_avg": 783.8823529411765
        },
        "formal_logic": {
            "num_questions_subject": 126,
            "accuracy": 0.4462809917355372,
            "num_readable_responses": 121,
            "input_tokens_avg": 1185.3636363636363
        },
        "global_facts": {
            "num_questions_subject": 100,
            "accuracy": 0.27586206896551724,
            "num_readable_responses": 29,
            "input_tokens_avg": 678.8620689655172
        },
        "high_school_biology": {
            "num_questions_subject": 310,
            "accuracy": 0.7322834645669292,
            "num_readable_responses": 254,
            "input_tokens_avg": 955.7362204724409
        },
        "high_school_chemistry": {
            "num_questions_subject": 203,
            "accuracy": 0.5584415584415584,
            "num_readable_responses": 77,
            "input_tokens_avg": 904.0
        },
        "high_school_computer_science": {
            "num_questions_subject": 100,
            "accuracy": 0.65625,
            "num_readable_responses": 64,
            "input_tokens_avg": 945.3125
        },
        "high_school_european_history": {
            "num_questions_subject": 165,
            "accuracy": 0.7115384615384616,
            "num_readable_responses": 104,
            "input_tokens_avg": 3408.2884615384614
        },
        "high_school_geography": {
            "num_questions_subject": 198,
            "accuracy": 0.8051948051948052,
            "num_readable_responses": 154,
            "input_tokens_avg": 620.5194805194806
        },
        "high_school_government_and_politics": {
            "num_questions_subject": 193,
            "accuracy": 0.8488372093023255,
            "num_readable_responses": 172,
            "input_tokens_avg": 850.6104651162791
        },
        "high_school_macroeconomics": {
            "num_questions_subject": 390,
            "accuracy": 0.5807453416149069,
            "num_readable_responses": 322,
            "input_tokens_avg": 800.0341614906832
        },
        "high_school_mathematics": {
            "num_questions_subject": 270,
            "accuracy": 0.4375,
            "num_readable_responses": 16,
            "input_tokens_avg": 805.75
        },
        "high_school_microeconomics": {
            "num_questions_subject": 238,
            "accuracy": 0.6179775280898876,
            "num_readable_responses": 178,
            "input_tokens_avg": 733.5112359550562
        },
        "high_school_physics": {
            "num_questions_subject": 151,
            "accuracy": 0.2727272727272727,
            "num_readable_responses": 44,
            "input_tokens_avg": 1060.409090909091
        },
        "high_school_psychology": {
            "num_questions_subject": 545,
            "accuracy": 0.8173913043478261,
            "num_readable_responses": 460,
            "input_tokens_avg": 703.4760869565217
        },
        "high_school_statistics": {
            "num_questions_subject": 216,
            "accuracy": 0.5081967213114754,
            "num_readable_responses": 122,
            "input_tokens_avg": 1216.827868852459
        },
        "high_school_us_history": {
            "num_questions_subject": 204,
            "accuracy": 0.7905405405405406,
            "num_readable_responses": 148,
            "input_tokens_avg": 2777.4662162162163
        },
        "high_school_world_history": {
            "num_questions_subject": 237,
            "accuracy": 0.791907514450867,
            "num_readable_responses": 173,
            "input_tokens_avg": 3170.057803468208
        },
        "human_aging": {
            "num_questions_subject": 223,
            "accuracy": 0.655367231638418,
            "num_readable_responses": 177,
            "input_tokens_avg": 615.1694915254237
        },
        "human_sexuality": {
            "num_questions_subject": 131,
            "accuracy": 0.7128712871287128,
            "num_readable_responses": 101,
            "input_tokens_avg": 644.019801980198
        },
        "international_law": {
            "num_questions_subject": 121,
            "accuracy": 0.7608695652173914,
            "num_readable_responses": 92,
            "input_tokens_avg": 1046.3695652173913
        },
        "jurisprudence": {
            "num_questions_subject": 108,
            "accuracy": 0.6547619047619048,
            "num_readable_responses": 84,
            "input_tokens_avg": 770.452380952381
        },
        "logical_fallacies": {
            "num_questions_subject": 163,
            "accuracy": 0.7417218543046358,
            "num_readable_responses": 151,
            "input_tokens_avg": 795.2980132450331
        },
        "machine_learning": {
            "num_questions_subject": 112,
            "accuracy": 0.39603960396039606,
            "num_readable_responses": 101,
            "input_tokens_avg": 798.2079207920792
        },
        "management": {
            "num_questions_subject": 103,
            "accuracy": 0.8309859154929577,
            "num_readable_responses": 71,
            "input_tokens_avg": 508.5774647887324
        },
        "marketing": {
            "num_questions_subject": 234,
            "accuracy": 0.8471615720524017,
            "num_readable_responses": 229,
            "input_tokens_avg": 643.6288209606987
        },
        "medical_genetics": {
            "num_questions_subject": 100,
            "accuracy": 0.7346938775510204,
            "num_readable_responses": 49,
            "input_tokens_avg": 745.1020408163265
        },
        "miscellaneous": {
            "num_questions_subject": 783,
            "accuracy": 0.8185483870967742,
            "num_readable_responses": 496,
            "input_tokens_avg": 522.0584677419355
        },
        "moral_disputes": {
            "num_questions_subject": 346,
            "accuracy": 0.6320474777448071,
            "num_readable_responses": 337,
            "input_tokens_avg": 719.4213649851632
        },
        "moral_scenarios": {
            "num_questions_subject": 895,
            "accuracy": 0.33884297520661155,
            "num_readable_responses": 847,
            "input_tokens_avg": 1109.2951593860685
        },
        "nutrition": {
            "num_questions_subject": 306,
            "accuracy": 0.7258687258687259,
            "num_readable_responses": 259,
            "input_tokens_avg": 697.5714285714286
        },
        "philosophy": {
            "num_questions_subject": 311,
            "accuracy": 0.7032258064516129,
            "num_readable_responses": 310,
            "input_tokens_avg": 826.3064516129032
        },
        "prehistory": {
            "num_questions_subject": 324,
            "accuracy": 0.7622950819672131,
            "num_readable_responses": 244,
            "input_tokens_avg": 807.0901639344262
        },
        "professional_accounting": {
            "num_questions_subject": 282,
            "accuracy": 0.38636363636363635,
            "num_readable_responses": 88,
            "input_tokens_avg": 1168.6022727272727
        },
        "professional_law": {
            "num_questions_subject": 1534,
            "accuracy": 0.39372138652714195,
            "num_readable_responses": 1529,
            "input_tokens_avg": 2163.7782864617398
        },
        "professional_medicine": {
            "num_questions_subject": 272,
            "accuracy": 0.6379310344827587,
            "num_readable_responses": 232,
            "input_tokens_avg": 1637.4568965517242
        },
        "professional_psychology": {
            "num_questions_subject": 612,
            "accuracy": 0.6176991150442478,
            "num_readable_responses": 565,
            "input_tokens_avg": 996.9185840707964
        },
        "public_relations": {
            "num_questions_subject": 110,
            "accuracy": 0.7241379310344828,
            "num_readable_responses": 87,
            "input_tokens_avg": 667.9080459770115
        },
        "security_studies": {
            "num_questions_subject": 245,
            "accuracy": 0.7213930348258707,
            "num_readable_responses": 201,
            "input_tokens_avg": 1650.8507462686566
        },
        "sociology": {
            "num_questions_subject": 201,
            "accuracy": 0.8540145985401459,
            "num_readable_responses": 137,
            "input_tokens_avg": 773.8686131386861
        },
        "us_foreign_policy": {
            "num_questions_subject": 100,
            "accuracy": 0.8181818181818182,
            "num_readable_responses": 88,
            "input_tokens_avg": 829.5795454545455
        },
        "virology": {
            "num_questions_subject": 166,
            "accuracy": 0.5367647058823529,
            "num_readable_responses": 136,
            "input_tokens_avg": 831.3235294117648
        },
        "world_religions": {
            "num_questions_subject": 171,
            "accuracy": 0.80625,
            "num_readable_responses": 160,
            "input_tokens_avg": 541.05625
        }
    }
}